{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL SOLUTION (RUN BELOW 2 CELLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "def extract_landmarks_from_frame(frame):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        for idx, lmk in enumerate(results.pose_landmarks.landmark):\n",
    "            # Increase weightage for hands and legs\n",
    "            if idx in [15, 17, 19, 21, 13, 16, 18, 20, 22, 14]:\n",
    "                landmarks.append([5*lmk.x, 5*lmk.y, lmk.z])\n",
    "            else:\n",
    "                landmarks.append([lmk.x, lmk.y, lmk.z])\n",
    "    else:\n",
    "        print(\"No pose landmarks detected in the frame.\")\n",
    "    landmarks = np.array(landmarks).flatten()\n",
    "\n",
    "    return np.array(landmarks)\n",
    "\n",
    "def resize_frame(frame, target_width):\n",
    "    height, width = frame.shape[:2]\n",
    "    aspect_ratio = width / height\n",
    "    target_height = int(target_width / aspect_ratio)\n",
    "    return cv2.resize(frame, (target_width, target_height))\n",
    "\n",
    "def compare_landmarks(landmarks_1, landmarks_2):\n",
    "    # Make sure both arrays have the same length\n",
    "  \n",
    "    min_length = min(len(landmarks_1), len(landmarks_2))\n",
    "    landmarks_1 = landmarks_1[:min_length]\n",
    "    landmarks_2 = landmarks_2[:min_length]\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity = np.dot(landmarks_1.flatten(), landmarks_2.flatten()) / \\\n",
    "                 (np.linalg.norm(landmarks_1) * np.linalg.norm(landmarks_2))\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gesture_image_path = \"gesture_image.png\"\n",
    "gesture_frame = cv2.imread(gesture_image_path)\n",
    "gesture_landmarks = extract_landmarks_from_frame(gesture_frame)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "record_video = input(\"Do you want to record a test video? (y/n): \").lower() == 'y'\n",
    "\n",
    "if record_video:\n",
    "    video_output_path = \"user_recorded_test_video.mp4\"\n",
    "    frame_width = 640\n",
    "    frame_height = 480\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(video_output_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera (index 0)\n",
    "    print(\"Recording video. Press 'q' to stop recording.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"Recording\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    test_video_path = \"user_recorded_test_video.mp4\"\n",
    "\n",
    "else:\n",
    "    test_video_path = \"test_video_waving.mp4\"\n",
    "\n",
    "test_video = cv2.VideoCapture(test_video_path)\n",
    "threshold = float(input(\"Enter number between 0 and 1 for precision: \"))\n",
    "\n",
    "frame_width = int(test_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "target_width = 640  # Set the target width for resizing\n",
    "output_path = \"output_vid_final.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (target_width, frame_height))\n",
    "\n",
    "while test_video.isOpened():\n",
    "    ret, frame = test_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = resize_frame(frame, target_width)  # Resize frame to fit within target width\n",
    "\n",
    "    landmarks = extract_landmarks_from_frame(frame)\n",
    "    if landmarks is not None:\n",
    "        similarity = compare_landmarks(landmarks, gesture_landmarks)\n",
    "        if similarity > threshold:\n",
    "            cv2.putText(frame, \"DETECTED\", (target_width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"Output Video\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "test_video.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using OpenCV MatchTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_gesture(frame, gesture_template):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    result = cv2.matchTemplate(gray_frame, gesture_template, cv2.TM_CCOEFF_NORMED)\n",
    "    threshold = 0.5\n",
    "    locations = np.where(result >= threshold)\n",
    "    if locations[0].size > 0:\n",
    "        return True, (locations[1][0], locations[0][0])\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "def annotate_frame(frame):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = 'DETECTED'\n",
    "    position = (frame.shape[1] - 150, 50)\n",
    "    font_scale = 1\n",
    "    font_color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.putText(frame, text, position, font, font_scale, font_color, thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "def compute_cosine_similarity(frame, gesture_template):\n",
    "    if gesture_template is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Flatten the vectors\n",
    "    vector1 = frame.flatten()\n",
    "    vector2 = gesture_template.flatten()\n",
    "    \n",
    "    # Compute dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Compute magnitudes of the vectors\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "# Read the test video\n",
    "test_video = cv2.VideoCapture('test_video2.mp4')\n",
    "\n",
    "# Read the gesture image\n",
    "gesture_template = cv2.imread('skipping_gesture_image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Get the dimensions of the test video frames\n",
    "frame_width = int(test_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Resize the gesture template image to match the dimensions of the video frames\n",
    "gesture_template = cv2.resize(gesture_template, (frame_width, frame_height))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "input_frame_rate = 10\n",
    "output_video = cv2.VideoWriter('output_video.mp4', fourcc, input_frame_rate, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = test_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Compute similarity\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    similarity = compute_cosine_similarity(gray_frame, gesture_template)\n",
    "\n",
    "    threshold = 0.5\n",
    "    if similarity > threshold:\n",
    "        frame = annotate_frame(frame)\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything when done\n",
    "test_video.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Compare each frame of test video with each frame of gesture video\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     ret_gesture, gesture_frame \u001b[38;5;241m=\u001b[39m \u001b[43mgesture_video\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret_gesture:\n\u001b[0;32m     69\u001b[0m         gesture_video\u001b[38;5;241m.\u001b[39mset(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_FRAMES, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Reset gesture video to beginning\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_gesture(frame, gesture_template):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    result = cv2.matchTemplate(gray_frame, gesture_template, cv2.TM_CCOEFF_NORMED)\n",
    "    threshold = 0.8\n",
    "    locations = np.where(result >= threshold)\n",
    "    if locations[0].size > 0:\n",
    "        return True, (locations[1][0], locations[0][0])\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "def annotate_frame(frame):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = 'DETECTED'\n",
    "    position = (frame.shape[1] - 150, 50)\n",
    "    font_scale = 1\n",
    "    font_color = (0, 255, 0)\n",
    "    thickness = 2\n",
    "    cv2.putText(frame, text, position, font, font_scale, font_color, thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "def compute_cosine_similarity(frame, gesture_template):\n",
    "    # Flatten the vectors\n",
    "    vector1 = frame.flatten()\n",
    "    vector2 = gesture_template.flatten()\n",
    "    \n",
    "    # Compute dot product\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    \n",
    "    # Compute magnitudes of the vectors\n",
    "    magnitude1 = np.linalg.norm(vector1)\n",
    "    magnitude2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "# Open test video\n",
    "test_video = cv2.VideoCapture('test_video.mp4')\n",
    "\n",
    "# Open gesture video\n",
    "gesture_video = cv2.VideoCapture('test_video copy.mp4')\n",
    "\n",
    "# Read the first frame of the gesture video and use it as a template\n",
    "ret, gesture_frame = gesture_video.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read gesture video.\")\n",
    "    exit()\n",
    "\n",
    "gesture_template = cv2.cvtColor(gesture_frame, cv2.COLOR_BGR2GRAY)\n",
    "input_frame_rate = test_video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('output_video.mp4', fourcc, 10.0, (int(test_video.get(3)), int(test_video.get(4))))\n",
    "\n",
    "while True:\n",
    "    ret, frame = test_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Compare each frame of test video with each frame of gesture video\n",
    "    while True:\n",
    "        ret_gesture, gesture_frame = gesture_video.read()\n",
    "        if not ret_gesture:\n",
    "            gesture_video.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset gesture video to beginning\n",
    "            break\n",
    "        \n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gesture_template = cv2.cvtColor(gesture_frame, cv2.COLOR_BGR2GRAY)\n",
    "        similarity = compute_cosine_similarity(gray_frame, gesture_template)\n",
    "\n",
    "        threshold = 0.5  # Adjust threshold as needed\n",
    "        if similarity > threshold:\n",
    "            frame = annotate_frame(frame)\n",
    "            break  # If gesture detected, no need to compare with remaining frames of gesture video\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    output_video.write(frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything when done\n",
    "test_video.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Display the output video\n",
    "output_video = cv2.VideoCapture('output_video.mp4')\n",
    "while output_video.isOpened():\n",
    "    ret, frame = output_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow('Output Video', frame)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BG Removal (dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the video\n",
    "video_path = 'test_video2.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_path = 'preprocessed_test_video.mp4'\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Apply background subtraction\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "    \n",
    "    # Apply morphological operations for noise reduction\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, None)\n",
    "    \n",
    "    # Apply thresholding to get binary mask\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 128, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Invert the mask\n",
    "    fg_mask = cv2.bitwise_not(fg_mask)\n",
    "    \n",
    "    # Apply the mask to extract the foreground\n",
    "    foreground = cv2.bitwise_and(frame, frame, mask=fg_mask)\n",
    "    \n",
    "    # Find contours in the foreground mask\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        # Get the bounding box of the largest contour (assuming it's the person)\n",
    "        bounding_box = cv2.boundingRect(contours[0])\n",
    "        x, y, w, h = bounding_box\n",
    "        \n",
    "        # Calculate the center of the bounding box\n",
    "        center_x = x + w // 2\n",
    "        center_y = y + h // 2\n",
    "        \n",
    "        # Calculate the shift needed to center the person\n",
    "        shift_x = frame.shape[1] // 2 - center_x\n",
    "        shift_y = frame.shape[0] // 2 - center_y\n",
    "        \n",
    "        # Shift the foreground image to center the person\n",
    "        foreground_centered = cv2.warpAffine(foreground, \n",
    "                                              M=np.float32([[1, 0, shift_x], [0, 1, shift_y]]), \n",
    "                                              dsize=(frame.shape[1], frame.shape[0]))\n",
    "        \n",
    "        # Write the centered foreground frame to the output video\n",
    "        out.write(foreground_centered)\n",
    "    else:\n",
    "        # If no contours found, just write the original foreground frame\n",
    "        out.write(foreground)\n",
    "    \n",
    "    cv2.imshow('Centered Foreground', foreground_centered)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using YOLO for Detection first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "output_layers_indices = [layer_names.index(layer) for layer in output_layers_names]\n",
    "output_layers = [layer_names[i] for i in output_layers_indices]\n",
    "\n",
    "video_capture = cv2.VideoCapture('test_video2.mp4')\n",
    "frame_count = 0\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('YOLO_preprocess_video.mp4', fourcc, 30.0, (int(video_capture.get(3)), int(video_capture.get(4))))\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    output_video.write(frame)\n",
    "\n",
    "video_capture.release()\n",
    "output_video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now there can be two approaches here:-\n",
    " 1. I can take two similarities one of bounding box coordinates and other one of the grayscaled content inside the bounding box (which might be unecessarily complex)\n",
    " 2. Or simply MatchTemplate each frame of gesture and test video, if the gesture_video is considerably long we can take 10-20 sample frames and compare each frame with the whole test video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_gesture(gesture_frame, test_frame):\n",
    "    # Convert frames to grayscale\n",
    "    gesture_gray = cv2.cvtColor(gesture_frame, cv2.COLOR_BGR2GRAY)\n",
    "    test_gray = cv2.cvtColor(test_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute the absolute difference between the frames\n",
    "    diff = cv2.absdiff(gesture_gray, test_gray)\n",
    "    \n",
    "    # Threshold the difference image\n",
    "    _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours in the thresholded image\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # If contours are found, consider it a match\n",
    "    if len(contours) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "output_layers_indices = [layer_names.index(layer) for layer in output_layers_names]\n",
    "output_layers = [layer_names[i] for i in output_layers_indices]\n",
    "\n",
    "# Load video\n",
    "video_capture = cv2.VideoCapture('test_video2.mp4')\n",
    "\n",
    "# Load gesture frame\n",
    "gesture_frame = cv2.imread('gesture_frame.jpg')\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "detected_flag = False\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('YOLO_DETECTED_video.mp4', fourcc, 30.0, (int(video_capture.get(3)), int(video_capture.get(4))))\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    if detect_gesture(gesture_frame, frame):\n",
    "        detected_flag = True\n",
    "        cv2.putText(frame, 'DETECTED', (width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    if detected_flag:\n",
    "        cv2.putText(frame, 'DETECTED', (width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    output_video.write(frame)\n",
    "\n",
    "video_capture.release()\n",
    "output_video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sample Frames from gesture video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "def sample_gesture_video(gesture_video_path):\n",
    "\n",
    "    gesture_video = cv2.VideoCapture(gesture_video_path)\n",
    "\n",
    "    gesture_frames = []\n",
    "\n",
    "    num_frames_to_sample = 10\n",
    "    total_frames_seen = 0\n",
    "\n",
    "    while len(gesture_frames) < num_frames_to_sample:\n",
    "        ret, frame = gesture_video.read()\n",
    "        total_frames_seen += 1\n",
    "        \n",
    "        if ret:\n",
    "            if random.random() < num_frames_to_sample / total_frames_seen:\n",
    "                gesture_frames.append(frame)\n",
    "\n",
    "    gesture_video.release()\n",
    "\n",
    "    # Get the dimensions of the video frames\n",
    "    height, width, _ = gesture_frames[0].shape\n",
    "\n",
    "    # Define the output video codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('gesture_sample_frames.mp4', fourcc, 3.0, (width, height))\n",
    "\n",
    "    # Write sampled frames to the output video\n",
    "    for frame in gesture_frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Frame Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_video_with_YOLO(video_path):\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    classes = []\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    output_layers_indices = [layer_names.index(layer) for layer in output_layers_names]\n",
    "    output_layers = [layer_names[i] for i in output_layers_indices]\n",
    "\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    processed_frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        height, width, channels = frame.shape\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5 and class_id == 0:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                processed_frames.append(frame)\n",
    "\n",
    "    video_capture.release()\n",
    "\n",
    "    return processed_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_frames = process_video_with_YOLO(\"test_video2.mp4\")\n",
    "output_video_path = \"yolo_test_video2.mp4\"\n",
    "frame_width, frame_height = processed_frames[0].shape[1], processed_frames[0].shape[0]\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "for frame in processed_frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_image_with_YOLO(image_path):\n",
    "    net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "    classes = []\n",
    "    with open(\"coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    output_layers_indices = [layer_names.index(layer) for layer in output_layers_names]\n",
    "    output_layers = [layer_names[i] for i in output_layers_indices]\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:  # We consider only the class \"person\"\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Process the image\n",
    "input_image_path = \"skipping_gesture_image.png\"\n",
    "output_image = process_image_with_YOLO(input_image_path)\n",
    "\n",
    "# Save the output image\n",
    "output_image_path = \"yolo_skipping_gesture_image.png\"\n",
    "cv2.imwrite(output_image_path, output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_frames = process_video_with_YOLO('test_video2.mp4')\n",
    "len(processed_frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Mediapipe pose landmark detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "def extract_landmarks_from_frame(frame):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    landmarks = None\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = np.array([[lmk.x, lmk.y, lmk.z] for lmk in results.pose_landmarks.landmark]).flatten()\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "def save_gesture_landmarks(landmarks, save_path):\n",
    "    np.save(save_path, landmarks)\n",
    "\n",
    "def load_gesture_landmarks(load_path):\n",
    "    return np.load(load_path)\n",
    "\n",
    "def compare_landmarks(landmarks_1, landmarks_2):\n",
    "    # Make sure both arrays have the same length\n",
    "    min_length = min(len(landmarks_1), len(landmarks_2))\n",
    "    landmarks_1 = landmarks_1[:min_length]\n",
    "    landmarks_2 = landmarks_2[:min_length]\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity = np.dot(landmarks_1.flatten(), landmarks_2.flatten()) / \\\n",
    "                 (np.linalg.norm(landmarks_1) * np.linalg.norm(landmarks_2))\n",
    "\n",
    "    return similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gesture_image_path = \"skipping_gesture_image.jpg\"\n",
    "gesture_frame = cv2.imread(gesture_image_path)\n",
    "gesture_landmarks = extract_landmarks_from_frame(gesture_frame)\n",
    "save_path = \"gesture_landmarks.npy\"\n",
    "save_gesture_landmarks(gesture_landmarks, save_path)\n",
    "\n",
    "gesture_landmarks = load_gesture_landmarks(save_path)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "test_video_path = \"test_video2.mp4\"\n",
    "test_video = cv2.VideoCapture(test_video_path)\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "frame_width = int(test_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_path = \"mp_output_vid.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "while test_video.isOpened():\n",
    "    ret, frame = test_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    landmarks = extract_landmarks_from_frame(frame)\n",
    "\n",
    "    if landmarks is not None:\n",
    "        similarity = compare_landmarks(landmarks, gesture_landmarks)\n",
    "\n",
    "        if similarity > threshold:\n",
    "            cv2.putText(frame, \"DETECTED\", (frame_width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "    cv2.imshow(\"Output Video\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "test_video.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added function of user recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "def extract_landmarks_from_frame(frame):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        \n",
    "        for idx, lmk in enumerate(results.pose_landmarks.landmark):\n",
    "            # Increase weightage for hands\n",
    "            if idx in [15, 17, 19, 21, 13, 16, 18, 20, 22, 14]:\n",
    "                landmarks.append([5*lmk.x , 5*lmk.y , lmk.z])\n",
    "            else:\n",
    "                landmarks.append([0.1*lmk.x, 0.1*lmk.y, lmk.z])\n",
    "        landmarks = np.array(landmarks).flatten()\n",
    "        \n",
    "\n",
    "    return np.array(landmarks) \n",
    "\n",
    "\n",
    "\n",
    "def save_gesture_landmarks(landmarks, save_path):\n",
    "    np.save(save_path, landmarks)\n",
    "\n",
    "def load_gesture_landmarks(load_path):\n",
    "    return np.load(load_path)\n",
    "\n",
    "def compare_landmarks(landmarks_1, landmarks_2):\n",
    "    # Make sure both arrays have the same length\n",
    "  \n",
    "    min_length = min(len(landmarks_1), len(landmarks_2))\n",
    "    landmarks_1 = landmarks_1[:min_length]\n",
    "    landmarks_2 = landmarks_2[:min_length]\n",
    "\n",
    "    # Compute similarity\n",
    "    similarity = np.dot(landmarks_1.flatten(), landmarks_2.flatten()) / \\\n",
    "                 (np.linalg.norm(landmarks_1) * np.linalg.norm(landmarks_2))\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth Magdum\\AppData\\Local\\Temp\\ipykernel_20120\\3140690484.py:42: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  similarity = np.dot(landmarks_1.flatten(), landmarks_2.flatten()) / \\\n"
     ]
    }
   ],
   "source": [
    "gesture_image_path = \"wave_gesture.png\"\n",
    "gesture_frame = cv2.imread(gesture_image_path)\n",
    "gesture_landmarks = extract_landmarks_from_frame(gesture_frame)\n",
    "# save_path = \"gesture_landmarks.npy\"\n",
    "# save_gesture_landmarks(gesture_landmarks, save_path)\n",
    "\n",
    "# gesture_landmarks = load_gesture_landmarks(save_path)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "record_video = input(\"Do you want to record a test video? (y/n): \").lower() == 'y'\n",
    "\n",
    "if record_video:\n",
    "    video_output_path = \"user_recorded_test_video.mp4\"\n",
    "    frame_width = 640\n",
    "    frame_height = 480\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(video_output_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera (index 0)\n",
    "    print(\"Recording video. Press 'q' to stop recording.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"Recording\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    test_video_path = \"user_recorded_test_video.mp4\"\n",
    "\n",
    "else:\n",
    "    test_video_path = \"waving_test_video.mp4\"\n",
    "\n",
    "test_video = cv2.VideoCapture(test_video_path)\n",
    "#Set threshold as needed\n",
    "threshold = float(input(\"Enter number between 0 and 1 for precision: \"))\n",
    "\n",
    "frame_width = int(test_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "output_path = \"output_vid_final.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "while test_video.isOpened():\n",
    "    ret, frame = test_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    landmarks = extract_landmarks_from_frame(frame)\n",
    "    if landmarks is not None:\n",
    "        similarity = compare_landmarks(landmarks, gesture_landmarks)\n",
    "        if similarity > threshold:\n",
    "            cv2.putText(frame, \"DETECTED\", (frame_width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"Output Video\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "test_video.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gesture_image_path = \"gesture_image.png\"\n",
    "gesture_frame = cv2.imread(gesture_image_path)\n",
    "gesture_landmarks = extract_landmarks_from_frame(gesture_frame)\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "record_video = input(\"Do you want to record a test video? (y/n): \").lower() == 'y'\n",
    "\n",
    "if record_video:\n",
    "    video_output_path = \"user_recorded_test_video.mp4\"\n",
    "    frame_width = 640\n",
    "    frame_height = 480\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(video_output_path, fourcc, 30.0, (frame_width, frame_height))\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Open default camera (index 0)\n",
    "    print(\"Recording video. Press 'q' to stop recording.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        cv2.imshow(\"Recording\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    test_video_path = \"user_recorded_test_video.mp4\"\n",
    "\n",
    "else:\n",
    "    test_video_path = \"test_video_waving.mp4\"\n",
    "\n",
    "test_video = cv2.VideoCapture(test_video_path)\n",
    "threshold = float(input(\"Enter number between 0 and 1 for precision: \"))\n",
    "\n",
    "frame_width = int(test_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(test_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "target_width = 640  # Set the target width for resizing\n",
    "output_path = \"output_vid_final.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (target_width, frame_height))\n",
    "\n",
    "while test_video.isOpened():\n",
    "    ret, frame = test_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = resize_frame(frame, target_width)  # Resize frame to fit within target width\n",
    "\n",
    "    landmarks = extract_landmarks_from_frame(frame)\n",
    "    if landmarks is not None:\n",
    "        similarity = compare_landmarks(landmarks, gesture_landmarks)\n",
    "        if similarity > threshold:\n",
    "            cv2.putText(frame, \"DETECTED\", (target_width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"Output Video\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "test_video.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
